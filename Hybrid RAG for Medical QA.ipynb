{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e886e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from typing import List,Tuple\n",
    "from hashlib import md5\n",
    "from ragas import evaluate\n",
    "from json import dumps, loads\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoTokenizer\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_ollama import OllamaLLM\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.chains import RetrievalQA\n",
    "from qdrant_client.models import HnswConfig\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain.chains import HypotheticalDocumentEmbedder\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.retrievers import ElasticSearchBM25Retriever\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from qdrant_client.conversions import common_types as RestToGrpc\n",
    "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.self_query.qdrant import QdrantTranslator\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain.prompts import ChatMessagePromptTemplate,PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,TokenTextSplitter\n",
    "from langchain.schema.runnable import RunnableMap,RunnableLambda, RunnablePassthrough\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.document_transformers import EmbeddingsClusteringFilter,EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline,LLMChainExtractor\n",
    "from langchain.chains.query_constructor.base import StructuredQueryOutputParser, get_query_constructor_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import evaluate as ev\n",
    "from ranx import evaluate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.metrics import BleuScore, ExactMatch, RougeScore, ResponseRelevancy, Faithfulness\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "evaluator_llm        = LangchainLLMWrapper(OllamaLLM(model=\"gemma3:1b\") )\n",
    "embeddings           = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",model_kwargs={\"device\": \"cpu\"})\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "bleu_metric   = BleuScore()\n",
    "em_metric     = ExactMatch()\n",
    "rouge_metric  = RougeScore()\n",
    "meteor_metric = ev.load(\"meteor\")\n",
    "bertscore     = ev.load(\"bertscore\")\n",
    "FF            = Faithfulness(llm=evaluator_llm)\n",
    "Relevancy     = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b42693",
   "metadata": {},
   "source": [
    "# Embedding and Generative Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer       = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c05a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\",num_gpu=1)\n",
    "llm_model       = OllamaLLM(model=\"gpt-oss:20b\", num_gpu=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b38d54",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4adcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path     = \"Medical_book.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f820103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "document = PDFPlumberLoader(path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = document[14:635]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0fd5a",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c148730",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = []\n",
    "\n",
    "def preprocess_page(doc: Document) -> Document:\n",
    "    text = doc.page_content\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    text = re.sub(r'\\n?\\s*Page\\s*\\d+\\s*\\n?', '\\n', text, flags=re.IGNORECASE)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = unidecode(text)\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    joined_lines = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if len(line) < 60 and not line.endswith(('.', '?', '!', ':')):\n",
    "            buffer += line + \" \"\n",
    "        else:\n",
    "            buffer += line\n",
    "            joined_lines.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "    if buffer:\n",
    "        joined_lines.append(buffer.strip())\n",
    "\n",
    "    text = '\\n'.join(joined_lines)\n",
    "\n",
    "    metadata[\"source\"] = \"Medical_book\"\n",
    "    metadata[\"page_number\"] = metadata.get(\"page\", \"N/A\")\n",
    "    metadata[\"length\"] = len(text)\n",
    "    metadata[\"hash\"] = md5(text.encode()).hexdigest()\n",
    "\n",
    "    return Document(page_content=text, metadata=metadata)\n",
    "\n",
    "seen_hashes = set()\n",
    "\n",
    "for doc in docs:\n",
    "    processed_doc = preprocess_page(doc)\n",
    "    if processed_doc.metadata[\"hash\"] not in seen_hashes and len(processed_doc.page_content.strip()) > 20:\n",
    "        preprocessed_docs.append(processed_doc)\n",
    "        seen_hashes.add(processed_doc.metadata[\"hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ad7d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocessed_docs[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18f1a8",
   "metadata": {},
   "source": [
    "# Splitter and Chucking Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9124fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_chunks = RecursiveCharacterTextSplitter(chunk_size=1500,chunk_overlap=200,separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]).split_documents(preprocessed_docs)\n",
    "chunks   = TokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer,chunk_size=800,chunk_overlap=200).split_documents(R_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(chunks):\n",
    "    num_tokens = len(tokenizer(doc.page_content)[\"input_ids\"])\n",
    "    if num_tokens > 1024:\n",
    "        print(f\"Chunk {i} is too long: {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4367363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Split blog post into {len(chunks)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62ec21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chunks[9].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data = [\n",
    "    {\n",
    "        \"Chunk\": i+1,\n",
    "        \"Page\": doc.metadata.get(\"page\", \"N/A\"),\n",
    "        \"Start Index\": doc.metadata.get(\"start_index\", \"N/A\"),\n",
    "        \"Length\": len(doc.page_content),\n",
    "        \"Preview\": doc.page_content[:200]\n",
    "    }\n",
    "    for i, doc in enumerate(chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc811927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61930a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd3ca3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a81f6f",
   "metadata": {},
   "source": [
    "# Dense DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba5cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qdrant_DB = Qdrant.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\"http://localhost:6333\",\n",
    "    collection_name=\"medical_data_denses\",\n",
    "    prefer_grpc=False,\n",
    "    hnsw_config={\n",
    "        \"m\": 16,\n",
    "        \"ef_construct\": 128,\n",
    "        \"full_scan_threshold\": 10000\n",
    "    },\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6692a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333,timeout=120.0)\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f38d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optional thing\n",
    "snapshot = client.create_snapshot(collection_name=\"medical_data_denses\")\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a3504",
   "metadata": {},
   "source": [
    "# Loading the Qdrant DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da57400",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdc100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore(client=client,collection_name=\"medical_data_denses\",embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73049bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dense         = \"What is Alzheimer’s disease?\"\n",
    "dense_results = qdrant.similarity_search_with_score(dense, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3352bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in dense_results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b5fe8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7dc82",
   "metadata": {},
   "source": [
    "# Spare DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_DB = ElasticsearchStore(\n",
    "                                embedding=embedding_model,\n",
    "                                index_name=\"medical_data_spares\",\n",
    "                                es_url=\"http://localhost:9200\", # http://localhost:9201\n",
    "                                strategy=ElasticsearchStore.BM25RetrievalStrategy(k1=1.2, b=0.75)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_DB.add_documents(documents=chunks, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f51f71",
   "metadata": {},
   "source": [
    "# Loading the ElasticSearch DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic   = ElasticsearchStore(\n",
    "                            embedding=embedding_model,\n",
    "                            index_name=\"medical_data_spares\",\n",
    "                            es_url=\"http://localhost:9200\",\n",
    "                            strategy=ElasticsearchStore.BM25RetrievalStrategy()\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "spare_retriever = elastic.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dense         = \"What is Alzheimer disease?\"\n",
    "dense_results = spare_retriever.invoke(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(dense_results):\n",
    "    print(f\"\\nResult {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384b511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff35f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_retreiver = qdrant.as_retriever(search_kwargs={\"k\": 3})\n",
    "spare_retriever       = elastic.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26d165",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4844b6",
   "metadata": {},
   "source": [
    "# Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "                                      retrievers=[vectorstore_retreiver,spare_retriever],\n",
    "                                      weights=[0.3, 0.7]  # Score fusion\n",
    "                                      ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08486664",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(query: str, k: int = 3) -> list[str]:\n",
    "    try:\n",
    "        results = ensemble_retriever.invoke(query) \n",
    "        return [doc.page_content for doc in results[:k]]  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3a127",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "es = query_embedding(\"What is Alzheimer disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04098af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm_model, chain_type=\"map_reduce\", retriever=ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8773ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = chain.invoke(\"What is Alzheimer disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a56f3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba601b4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468cc16",
   "metadata": {},
   "source": [
    "# Synthetic Question Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c488e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\\n\\n\".join(doc.page_content for doc in preprocessed_docs)\n",
    "\n",
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_chunks = RecursiveCharacterTextSplitter(chunk_size=1500,chunk_overlap=200,separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]).split_text(full_text)\n",
    "splitter = TokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer,chunk_size=800,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6693f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for chunk in R_chunks:\n",
    "    chunks.extend(splitter.split_text(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7397f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9380231",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain  = QAGenerationChain.from_llm(llm_model, max_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qa_pairs  = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"text\": chunk})\n",
    "        qa_pairs.extend(result[\"questions\"])  \n",
    "    except Exception as e:\n",
    "        print(f\"[Chunk {i}] Error generating QA: {e}\")\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "df.to_excel(\"Synthetic Question Generator.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d9b6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e50905",
   "metadata": {},
   "source": [
    "# Testing Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cddfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Synthetic Question Generator.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b377bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries      = df['user_query'].tolist()\n",
    "expected_responses  = df['ground_truth'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65829a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d08d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a03a7",
   "metadata": {},
   "source": [
    "# Testing Ensemble Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for query, reference in zip(sample_queries, expected_responses):\n",
    "    relevant_docs = query_embedding(query)  \n",
    "    response      = chain.invoke(query)\n",
    "    result        = response[\"result\"]\n",
    "\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": relevant_docs,\n",
    "        \"reference\": reference,\n",
    "        \"response\": result\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_excel(\"Ensemble Retriever.xlsx\", index=False)\n",
    "dataset = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372816b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099aaf80",
   "metadata": {},
   "source": [
    "# RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f975bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=[],\n",
    "                template=\"You are a helpful assistant that generates multiple search queries based on a single input query.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=[\"original_query\"],\n",
    "                template=\"Generate multiple search queries related to: {original_query} \\nOUTPUT (4 queries):\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (prompt | llm_model | StrOutputParser() | (lambda x: [q.strip() for q in x.split(\"\\n\") if q.strip()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbbcdc",
   "metadata": {},
   "source": [
    "### Reciprocal Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: List[List[Document]], k: int = 60) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Perform Reciprocal Rank Fusion on multiple ranked lists of Documents.\n",
    "    \n",
    "    Args:\n",
    "        results (List[List[Document]]): A list of ranked lists of Documents.\n",
    "        k (int): Rank-smoothing constant. Default is 60.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Document, float]]: Re-ranked list of unique Documents with fusion scores.\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc.dict(), sort_keys=True)\n",
    "            fused_scores[doc_str] = fused_scores.get(doc_str, 0) + 1 / (rank + 1 + k)\n",
    "    reranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [(Document.parse_obj(loads(doc_str)), score) for doc_str, score in reranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf_runnable    = RunnableLambda(lambda results: [doc for doc, _ in reciprocal_rank_fusion(results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragfusion_chain = generate_queries | RunnableLambda(lambda queries: [vectorstore_retreiver.invoke(q) for q in queries]) | rrf_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322869a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rff_query_embedding(query: str, k: int = 3) -> list[str]:\n",
    "    try:\n",
    "        results = ragfusion_chain.invoke(query) \n",
    "        return [doc.page_content for doc in results[:k]]  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768daa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rrf_content = rff_query_embedding(\"What is Alzheimer disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\":  ragfusion_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54507127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "op = full_rag_fusion_chain.invoke(\"What is Alzheimer disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ad23d",
   "metadata": {},
   "source": [
    "# Testing RAG Fusion with RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for query, reference in tqdm(zip(sample_queries, expected_responses)):\n",
    "    relevant_docs = rff_query_embedding(query)\n",
    "    result = full_rag_fusion_chain.invoke(query)\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": relevant_docs,\n",
    "        \"reference\": reference,\n",
    "        \"response\": result\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_excel(\"RAG Fusion with RRF.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063554ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10e7fe",
   "metadata": {},
   "source": [
    "# Contextual Compression Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8004d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters     = EmbeddingsRedundantFilter(embeddings=embedding_model) \n",
    "\n",
    "# Removes near-duplicate documents/chunks using embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45faac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_get_input(query: str, doc: Document) -> dict:\n",
    "    return {\"input_text\": doc.page_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42990d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_llm  = OllamaLLM(model=\"gemma3:1b\")\n",
    "\n",
    "prompt     = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{input_text}\")\n",
    "\n",
    "llm_chain  = LLMChain(llm=small_llm, prompt=prompt)\n",
    "\n",
    "summarizer = LLMChainExtractor(llm_chain=llm_chain,get_input=custom_get_input)  \n",
    "\n",
    "# LLMChainExtractor summarizes each document/chunk separately, not the entire document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordering = LongContextReorder() \n",
    "\n",
    "# Reorders the chunks to bring the most relevant ones earlier in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfeb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline   = DocumentCompressorPipeline(transformers=[filters, summarizer, reordering]) \n",
    "\n",
    "# Combines multiple transformers into a single pipeline think of it like a document preprocessing chain: \n",
    "\n",
    "#     i) Remove redundancy \n",
    "\n",
    "##   ii) Reorder for relevance\n",
    "\n",
    "### iii) summarize\n",
    "\n",
    "# This would summarize chunks before reordering. \n",
    "\n",
    "# Sometimes works better when summaries are uniform in length and easier to compare for ranking. \n",
    "\n",
    "# But generally, summarizing last is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70db411",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever_reordered = ContextualCompressionRetriever(\n",
    "                                                                 base_compressor = pipeline, \n",
    "                                                                 base_retriever  = vectorstore_retreiver,\n",
    "                                                                 search_kwargs   = {\"k\": 3, \"include_metadata\": True}\n",
    "                                                                 ) \n",
    "\n",
    "# Wraps the base retriever and applies the compression pipeline after retrieval but before passing context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = compression_retriever_reordered.invoke(\"What is Alzheimer disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaac771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRR_query_embedding(query: str, k: int = 3) -> list[str]:\n",
    "    try:\n",
    "        results = compression_retriever_reordered.invoke(query) \n",
    "        return [doc.page_content for doc in results[:k]]  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = CRR_query_embedding(\"What is Parkinson’s disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm_model, retriever=compression_retriever_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Alzheimer disease?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(query)['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for query, reference in tqdm(zip(sample_queries, expected_responses)):\n",
    "    relevant_docs = CRR_query_embedding(query)\n",
    "    result        = chain.invoke(query)['result']\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": relevant_docs,\n",
    "        \"reference\": reference,\n",
    "        \"response\": result\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_excel(\"Contextual Compression Retrieval.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99af729",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a487c",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde  = HypotheticalDocumentEmbedder.from_llm(llm = llm_model,base_embeddings = embedding_model, prompt_key = \"web_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f240a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query   = 'What is Alzheimer disease?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = qdrant.similarity_search_by_vector(hyde.embed_query(query), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_query_embedding(query: str, k: int = 3) -> list[str]:\n",
    "    try:\n",
    "        results = qdrant.similarity_search_by_vector(hyde.embed_query(query), k=3)\n",
    "        return [doc.page_content for doc in results[:k]]  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_op = hyde_query_embedding(\"What is Parkinson’s disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_embed_query = RunnableLambda(hyde.embed_query)\n",
    "\n",
    "qdrant_retriever = RunnableLambda(lambda vector: qdrant.similarity_search_by_vector(vector, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e35b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"question\": RunnablePassthrough(),  \n",
    "        \"context\" : hyde_embed_query | qdrant_retriever | RunnableLambda(format_docs)\n",
    "    })\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"What is Parkinson’s disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ec828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for query, reference in tqdm(zip(sample_queries, expected_responses)):\n",
    "    relevant_docs = hyde_query_embedding(query)\n",
    "    result        = rag_chain.invoke(query)\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": relevant_docs,\n",
    "        \"reference\": reference,\n",
    "        \"response\": result\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_excel(\"hyde.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7f314",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6d3d1",
   "metadata": {},
   "source": [
    "# Flash Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b11fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bab31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vectorstore_retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR_query_embedding(query: str, k: int = 3) -> list[str]:\n",
    "    try:\n",
    "        results = compression_retriever.invoke(query) \n",
    "        return [doc.page_content for doc in results[:k]]  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3434ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = FR_query_embedding(\"What is Parkinson’s disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8393235",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99909f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"What is Alzheimer disease?\"\n",
    "\n",
    "output = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b401b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for query, reference in tqdm(zip(sample_queries, expected_responses)):\n",
    "    relevant_docs = FR_query_embedding(query)\n",
    "    result        = chain.invoke(query)\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": relevant_docs,\n",
    "        \"reference\": reference,\n",
    "        \"response\": result\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_excel(\"Flash Reranker.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07023a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e81b7",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ef9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reteriver_metrics(df, model=SentenceTransformer(\"all-MiniLM-L6-v2\"), threshold=0.55, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate RAG retrieval performance with per-query precision, recall, and top-k ranking metrics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Must have columns 'reference' (string) and 'retrieved_contexts' (list of strings).\n",
    "        model: SentenceTransformer embedding model.\n",
    "        threshold (float): Cosine similarity threshold to count a retrieved doc as relevant.\n",
    "        k (int): Top-k for ranking metrics.\n",
    "\n",
    "    Returns:\n",
    "        dict: context-level precision, recall, and top-k ranking metrics.\n",
    "    \"\"\"\n",
    "    qrels = {}  # ground-truth relevance\n",
    "    run   = {}  # predicted similarity scores\n",
    "\n",
    "    per_query_precision = []\n",
    "    per_query_recall    = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        qid       = f\"q{idx}\"\n",
    "        reference = row[\"reference\"]\n",
    "        retrieved = row[\"retrieved_contexts\"]\n",
    "\n",
    "        if not isinstance(retrieved, list):\n",
    "            retrieved = [retrieved]\n",
    "\n",
    "        # Compute embeddings\n",
    "        ref_emb  = model.encode(reference, convert_to_tensor=True)\n",
    "        ctx_embs = model.encode(retrieved, convert_to_tensor=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        sims = util.cos_sim(ref_emb, ctx_embs).flatten().tolist()\n",
    "        sims = [float(s) for s in sims]\n",
    "\n",
    "        # Create qrels and run entries\n",
    "        rels = {f\"d{idx}_{i}\": int(sim > threshold) for i, sim in enumerate(sims)}\n",
    "        qrels[qid] = rels\n",
    "        run[qid]   = {f\"d{idx}_{i}\": float(sim) for i, sim in enumerate(sims)}\n",
    "\n",
    "        # Per-query precision & recall\n",
    "        relevant_count = sum(rels.values())\n",
    "        retrieved_count = len(retrieved)\n",
    "\n",
    "        # Precision = relevant retrieved / total retrieved\n",
    "        precision = relevant_count / retrieved_count if retrieved_count else 0\n",
    "\n",
    "        # Recall = 1 if at least one relevant doc retrieved, else 0 (binary relevance assumption)\n",
    "        recall = 1.0 if relevant_count > 0 else 0.0\n",
    "\n",
    "        per_query_precision.append(precision)\n",
    "        per_query_recall.append(recall)\n",
    "\n",
    "    # Context-level aggregated metrics\n",
    "    context_precision = sum(per_query_precision) / len(per_query_precision)\n",
    "    context_recall    = sum(per_query_recall) / len(per_query_recall)\n",
    "\n",
    "    # Top-k ranking metrics using Ranx\n",
    "    ranking_metrics = evaluate(\n",
    "        qrels,\n",
    "        run,\n",
    "        metrics=[f\"precision@{k}\", f\"recall@{k}\", f\"mrr@{k}\", f\"ndcg@{k}\"]\n",
    "    )\n",
    "\n",
    "    # Combine all results\n",
    "    final_results = {\n",
    "        \"context_precision\": context_precision,\n",
    "        \"context_recall\": context_recall,\n",
    "        **ranking_metrics\n",
    "    }\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "async def generator_Metrics(predictions, references, user_input, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Evaluate RAG predictions with multiple metrics and return a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): Generated answers.\n",
    "        references (list): Ground-truth answers.\n",
    "        user_input (list): Queries.\n",
    "        retrieved_docs (list): Retrieved contexts.\n",
    "        Relevancy, FF, bleu_metric, em_metric, rouge_metric: Ragas metric objects.\n",
    "        meteor_metric, bertscore: other metric objects.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all metrics per query.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        # Ensure reference is a string\n",
    "        ref_str = references[i] if isinstance(references[i], str) else references[i][0]\n",
    "\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=user_input[i],\n",
    "            response=predictions[i],\n",
    "            reference=ref_str,\n",
    "            retrieved_contexts=[ref_str]\n",
    "        )\n",
    "\n",
    "        # Evaluate all metrics asynchronously\n",
    "        relevancyy   = await Relevancy.single_turn_ascore(sample)\n",
    "        faith        = await FF.single_turn_ascore(sample)    \n",
    "        bleu_score   = await bleu_metric.single_turn_ascore(sample)\n",
    "        em_score     = await em_metric.single_turn_ascore(sample)\n",
    "        rouge_score  = await rouge_metric.single_turn_ascore(sample)\n",
    "        meteor_score = meteor_metric.compute(predictions=[predictions[i]], references=[ref_str])[\"meteor\"]\n",
    "        bertscore_results  = bertscore.compute(predictions=[predictions[i]], references=[ref_str], lang=\"en\")\n",
    "        hallucination_rate = 1 - faith\n",
    "\n",
    "        row = {\n",
    "            \"query\": user_input[i],\n",
    "            \"retrieved_doc\": retrieved_docs[i],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"reference\": ref_str,\n",
    "            \"BLEU\": bleu_score,\n",
    "            \"ExactMatch\": em_score,\n",
    "            \"ROUGE\": rouge_score,\n",
    "            \"BERTScore\": bertscore_results[\"f1\"][0],\n",
    "            \"METEOR\": meteor_score,\n",
    "            \"ResponseRelevancy\": relevancyy,\n",
    "            \"Faithfulness\": faith,\n",
    "            \"hallucination_rate\": hallucination_rate,\n",
    "        }\n",
    "        data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323e9f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES                       = pd.read_excel(\".\\excel files\\Ensemble Retriever.xlsx\")\n",
    "ES[\"retrieved_contexts\"] = ES[\"retrieved_contexts\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "RRF                       = pd.read_excel(\".\\excel files\\RAG Fusion with RRF.xlsx\")\n",
    "RRF[\"retrieved_contexts\"] = RRF[\"retrieved_contexts\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "CCR                       = pd.read_excel(\".\\excel files\\Contextual Compression Retrieval.xlsx\")\n",
    "CCR[\"retrieved_contexts\"] = CCR[\"retrieved_contexts\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "hyde                       = pd.read_excel(\".\\excel files\\hyde.xlsx\")\n",
    "hyde[\"retrieved_contexts\"] = hyde[\"retrieved_contexts\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "RR                       = pd.read_excel(\".\\excel files\\Flash Reranker.xlsx\")\n",
    "RR[\"retrieved_contexts\"] = RR[\"retrieved_contexts\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9588f",
   "metadata": {},
   "source": [
    "## Retriever Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"EnsembleRetriever\": reteriver_metrics(ES),\n",
    "    \"RAGFusionRRF\": reteriver_metrics(RRF),\n",
    "    \"ContextualCompression\": reteriver_metrics(CCR),\n",
    "    \"HyDE\": reteriver_metrics(hyde),\n",
    "    \"FlashReranker\": reteriver_metrics(RR)\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient=\"index\").reset_index()\n",
    "df_results.rename(columns={\"index\": \"Method\"}, inplace=True)\n",
    "df_results.to_excel(\"RAG_Retrieval_Metrics.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632affca",
   "metadata": {},
   "source": [
    "## Generator Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e4106",
   "metadata": {},
   "source": [
    "| **Metric**             | **Range**            | **Good When**      | **Bad When** | **Tip / Trick**                                                                                          |\n",
    "| ---------------------- | -------------------- | ------------------ | ------------ | -------------------------------------------------------------------------------------------------------- |\n",
    "| **BLEU**               | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Good for short, n-gram overlap (machine translation style). Works best if answers are short and factual. |\n",
    "| **Exact Match (EM)**   | 0 or 1 (sometimes %) | 🔼 Higher = Better | 🔽 Lower     | Strict metric — requires generated text = reference exactly. Only makes sense for factoid Q\\&A.          |\n",
    "| **ROUGE**              | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Focuses on recall/coverage. Good if you want the model to capture most of the important reference words. |\n",
    "| **BERTScore**          | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Semantic similarity (uses embeddings). Better than BLEU/ROUGE when wording differs but meaning is same.  |\n",
    "| **METEOR**             | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Balances precision & recall with synonym matching. More forgiving than BLEU.                             |\n",
    "| **Response Relevancy** | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Checks if answer is relevant to the query/context. Low = answer is off-topic.                            |\n",
    "| **Faithfulness**       | 0 → 1                | 🔼 Higher = Better | 🔽 Lower     | Checks if answer is grounded in retrieved docs. Low = hallucination risk.                                |\n",
    "| **Hallucination Rate** | 0 → 1                | 🔽 Lower = Better  | 🔼 Higher    | Measures how much the model \"makes up\" stuff. Ideally close to 0.                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db58ab9",
   "metadata": {},
   "source": [
    "##### EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ff019",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_user_input     = ES[\"user_input\"].tolist()\n",
    "ES_retrieved_docs = ES[\"retrieved_contexts\"].tolist()\n",
    "ES_predictions    = ES[\"response\"].tolist()\n",
    "ES_references     = ES[\"reference\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_metrics        = asyncio.run(generator_Metrics(ES_predictions,ES_references,ES_user_input,ES_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a535534",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae18c23",
   "metadata": {},
   "source": [
    "##### RAG Fusion RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdcc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "RRF_user_input     = RRF[\"user_input\"].tolist()\n",
    "RRF_retrieved_docs = RRF[\"retrieved_contexts\"].tolist()\n",
    "RRF_predictions    = RRF[\"response\"].tolist()\n",
    "RRF_references     = RRF[\"reference\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0279907",
   "metadata": {},
   "outputs": [],
   "source": [
    "RRF_metrics        = asyncio.run(generator_Metrics(RRF_predictions,RRF_references,RRF_user_input,RRF_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7df5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RRF_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791091a6",
   "metadata": {},
   "source": [
    "##### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCR_user_input     = CCR[\"user_input\"].tolist()\n",
    "CCR_retrieved_docs = CCR[\"retrieved_contexts\"].tolist()\n",
    "CCR_predictions    = CCR[\"response\"].tolist()\n",
    "CCR_references     = CCR[\"reference\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCR_metrics        = asyncio.run(generator_Metrics(CCR_predictions,CCR_references,CCR_user_input,CCR_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCR_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3bc65",
   "metadata": {},
   "source": [
    "##### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_user_input     = hyde[\"user_input\"].tolist()\n",
    "hyde_retrieved_docs = hyde[\"retrieved_contexts\"].tolist()\n",
    "hyde_predictions    = hyde[\"response\"].tolist()\n",
    "hyde_references     = hyde[\"reference\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123653af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_metrics        = asyncio.run(generator_Metrics(hyde_predictions,hyde_references,hyde_user_input,hyde_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5631a74",
   "metadata": {},
   "source": [
    "##### FlashReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_user_input     = RR[\"user_input\"].tolist()\n",
    "RR_retrieved_docs = RR[\"retrieved_contexts\"].tolist()\n",
    "RR_predictions    = RR[\"response\"].tolist()\n",
    "RR_references     = RR[\"reference\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_metrics        = asyncio.run(generator_Metrics(RR_predictions,RR_references,RR_user_input,RR_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d2b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RR_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multiple sheets in one Excel file\n",
    "with pd.ExcelWriter(\"RAG_Generation_Metrics.xlsx\") as writer:\n",
    "    ES_metrics.to_excel(writer, sheet_name=\"EnsembleRetriever\", index=False)\n",
    "    RRF_metrics.to_excel(writer, sheet_name=\"RAGFusionRRF\", index=False)\n",
    "    CCR_metrics.to_excel(writer, sheet_name=\"ContextualCompression\", index=False)\n",
    "    hyde_metrics.to_excel(writer, sheet_name=\"HyDE\", index=False)\n",
    "    RR_metrics.to_excel(writer, sheet_name=\"FlashReranker\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5a069",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b2a8f",
   "metadata": {},
   "source": [
    "we systematically evaluated five RAG retrieval approaches for **medical domain QA**:\n",
    "\n",
    "- **EnsembleRetriever** (Dense + Sparse fusion)  \n",
    "- **RAG Fusion (RRF)** (query expansion + reciprocal rank fusion)  \n",
    "- **Contextual Compression Retrieval (CCR)** (summarization + redundancy filtering)  \n",
    "- **HyDE** (hypothetical document generation)  \n",
    "- **Flash Reranker** (neural re-ranking on top of base retriever)  \n",
    "\n",
    "##  Retrieval Metrics\n",
    "From the `RAG_Retrieval_Metrics.xlsx` results:  \n",
    "- **HyDE** achieved the **highest context precision (1.0)** while maintaining perfect recall and ranking scores (`mrr@5 = 1.0`, `ndcg@5 = 1.0`).  \n",
    "- Other methods (EnsembleRetriever, RRF, CCR, FlashReranker) performed almost identically, with strong recall (1.0) but slightly lower precision (≈0.89).  \n",
    "\n",
    " **HyDE is the best retriever overall** because it not only retrieves relevant documents consistently but also reduces noise (higher precision).  \n",
    "\n",
    "##  Generation Metrics\n",
    "From the `RAG_Generation_Metrics.xlsx` results:  \n",
    "- **BLEU, ROUGE, METEOR** show how much generated answers overlap with ground-truth text.  \n",
    "- **BERTScore & Response Relevancy** confirm semantic alignment with the reference.  \n",
    "- **Faithfulness & Hallucination Rate** are critical in the medical domain. Models with higher faithfulness (closer to 1) and lower hallucination (<0.2) are more reliable.  \n",
    "\n",
    "##### Across all methods, **HyDE and Contextual Compression Retrieval** tend to give more **faithful and relevant responses**, while **Flash Reranker** improves ranking but sometimes introduces hallucinations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
